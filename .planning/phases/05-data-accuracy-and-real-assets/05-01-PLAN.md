---
phase: 05-data-accuracy-and-real-assets
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/scrape-season-records.js
  - scripts/scrape-top-performers.js
  - package.json
  - package-lock.json
autonomous: true
requirements:
  - DATA-01
  - DATA-02

must_haves:
  truths:
    - "Playwright is installed as standalone library (not @playwright/test)"
    - "scripts/scrape-season-records.js can be run with `node scripts/scrape-season-records.js` and produces scripts/scraped-season-records.json"
    - "scripts/scrape-top-performers.js can be run with `node scripts/scrape-top-performers.js` and produces scripts/scraped-top-performers.json"
    - "Both scripts follow the verify-then-apply pattern — they write JSON, never overwrite src/data/stats.ts directly"
  artifacts:
    - path: "scripts/scrape-season-records.js"
      provides: "Playwright scraper for KKR season-by-season records from iplt20.com/teams/kolkata-knight-riders/archive"
      exports: []
    - path: "scripts/scrape-top-performers.js"
      provides: "Playwright scraper for KKR top performers from ESPNcricinfo"
      exports: []
  key_links:
    - from: "scripts/scrape-season-records.js"
      to: "scripts/scraped-season-records.json"
      via: "fs.writeFileSync"
      pattern: "writeFileSync.*scraped-season-records"
    - from: "scripts/scrape-top-performers.js"
      to: "scripts/scraped-top-performers.json"
      via: "fs.writeFileSync"
      pattern: "writeFileSync.*scraped-top-performers"
---

<objective>
Install Playwright as a standalone scraping library and write two Node.js scraping scripts — one for KKR season records and one for KKR top performers. Each script writes to an intermediate JSON file for human review. This is pure tooling setup — no src/data files are modified yet.

Purpose: DATA-01 and DATA-02 require accurate IPL statistics sourced via Playwright. The scripts established here run in Plan 02 and produce JSON that gets human-reviewed before applying to stats.ts.
Output: `scripts/scrape-season-records.js`, `scripts/scrape-top-performers.js`, updated `package.json`
</objective>

<execution_context>
@/Users/mithra_sundaram/.claude/get-shit-done/workflows/execute-plan.md
@/Users/mithra_sundaram/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-data-accuracy-and-real-assets/05-RESEARCH.md

<interfaces>
<!-- Key types the scraping scripts must produce compatible JSON for. From src/types/stats.ts: -->

```typescript
export interface SeasonRecord {
  year: number;          // IPL year, e.g. 2008
  wins: number;
  losses: number;
  noResult: number;      // ties or no-result matches
  position: number;      // final standings position that season (1 = champion)
  title: boolean;        // true if KKR won the IPL that year
}

export interface TopPerformer {
  name: string;          // Player's full name
  category: "runs" | "wickets";
  total: number;         // career total runs or wickets for KKR
  seasons: number;       // seasons played for KKR
}
```

Current stats.ts data (to be replaced/corrected):
- 2021 season: position is 6 (WRONG — KKR were runners-up, position should be 2)
- 2025 season: MISSING entirely (KKR finished 8th, ~5 wins)
- Sunil Narine runs: 3500 (WRONG — actual ~1507 IPL runs)
- Gautam Gambhir: PRESENT in stats.ts at 2982 runs but likely wrong (verified ~3035 KKR runs)
- Robin Uthappa: 3145 runs (WRONG — actual ~2649 KKR runs)
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install Playwright library and set up scripts directory</name>
  <files>package.json, package-lock.json</files>
  <action>
Install Playwright as the standalone library (NOT @playwright/test — that is the test runner, a different package). Then install Chromium browser binary. Finally create the scripts/ directory structure.

Run these commands from the project root (/Users/mithra_sundaram/Desktop/code/AI/projects/team_site):

```bash
npm install --save-dev playwright
npx playwright install chromium
mkdir -p scripts/output
```

IMPORTANT: The package name is `playwright`, not `@playwright/test`. Verify package.json devDependencies contains `"playwright"` after install.

Do NOT install ts-node or any TypeScript runner — scripts will be plain JavaScript (.js) to avoid compilation step.
  </action>
  <verify>
    <automated>cd /Users/mithra_sundaram/Desktop/code/AI/projects/team_site && node -e "const { chromium } = require('playwright'); console.log('playwright ok:', typeof chromium.launch)"</automated>
  </verify>
  <done>Running `node -e "const { chromium } = require('playwright')"` succeeds without error. package.json devDependencies contains playwright entry.</done>
</task>

<task type="auto">
  <name>Task 2: Write scrape-season-records.js for iplt20.com KKR archive</name>
  <files>scripts/scrape-season-records.js</files>
  <action>
Create `/Users/mithra_sundaram/Desktop/code/AI/projects/team_site/scripts/scrape-season-records.js` as a standalone Node.js Playwright script.

The script must:
1. Launch Chromium in headless mode with a research-bot User-Agent
2. Navigate to `https://www.iplt20.com/teams/kolkata-knight-riders/archive` with `waitUntil: 'domcontentloaded'` and 30s timeout
3. Wait for a table or data element to appear (try `waitForSelector('table', { timeout: 15000 })` first; if that fails after 15s, fall back to `waitForSelector('.standings-table, .archive-table, [class*="table"]', { timeout: 10000 })`)
4. Extract ALL table rows (including headers) as raw text via `page.$$eval('table tr', ...)` — each row becomes an array of cell text values
5. Write the raw extracted data to `scripts/scraped-season-records.json` using `fs.writeFileSync` (path relative to project root: `'./scripts/scraped-season-records.json'`)
6. Log the number of rows found and "Review scripts/scraped-season-records.json before applying to stats.ts"
7. Handle errors with try/catch, log the error, call browser.close() in finally block, and exit with process.exit(1) on failure

The JSON output is intentionally RAW — it is not pre-parsed into SeasonRecord format. Human review in Plan 02 determines which rows are valid data and how to parse them. The output format is an array of objects: `{ cells: string[] }`.

Use the exact pattern from the research file's "Playwright Standalone Script — Full Pattern" code example. Do NOT use `waitUntil: 'networkidle'` (unreliable on SPAs). Do NOT use fixed `page.waitForTimeout()` calls.

File header comment must include: the target URL, the date, and a note that DOM selectors were determined by browser inspection of the live page.
  </action>
  <verify>
    <automated>cd /Users/mithra_sundaram/Desktop/code/AI/projects/team_site && node -e "const src = require('fs').readFileSync('scripts/scrape-season-records.js','utf8'); const checks = ['chromium.launch','domcontentloaded','scraped-season-records.json','writeFileSync','browser.close']; checks.forEach(c => { if(!src.includes(c)) throw new Error('Missing: '+c); }); console.log('script structure ok')"</automated>
  </verify>
  <done>scripts/scrape-season-records.js exists, contains all required Playwright patterns, and passes the structure check. File uses require('playwright') not require('@playwright/test').</done>
</task>

<task type="auto">
  <name>Task 3: Write scrape-top-performers.js for ESPNcricinfo KKR stats</name>
  <files>scripts/scrape-top-performers.js</files>
  <action>
Create `/Users/mithra_sundaram/Desktop/code/AI/projects/team_site/scripts/scrape-top-performers.js` as a standalone Node.js Playwright script.

The script must scrape KKR top performers from ESPNcricinfo. ESPNcricinfo returns 403 on plain fetch — Playwright with a realistic User-Agent is required.

The script must:
1. Launch Chromium in headless mode with User-Agent: `'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'`
2. Navigate to `https://stats.espncricinfo.com/ci/engine/records/batting/most_runs_career.html?id=335971;team=335971;type=team` with `waitUntil: 'domcontentloaded'` and 30s timeout — this is KKR's batting records page filtered by team
3. Wait for `waitForSelector('table.engineTable, table[class*="table"]', { timeout: 15000 })`
4. Extract the batting stats table rows as raw text arrays
5. Write to `scripts/scraped-top-performers-batting.json`
6. Then navigate to the bowling equivalent: `https://stats.espncricinfo.com/ci/engine/records/bowling/most_wickets_career.html?id=335971;team=335971;type=team`
7. Extract bowling table rows, write to `scripts/scraped-top-performers-bowling.json`
8. Log row counts for each and "Review both JSON files before applying to stats.ts"
9. Handle errors with try/catch, always close browser in finally

If ESPNcricinfo returns 403 or empty data (check: array length < 2 rows), log a clear warning: "ESPNcricinfo blocked — use Wikipedia or manual data for top performers. Check scraped JSON for empty/error content."

Output format for both files: `{ rows: [{ cells: string[] }] }` — raw, unprocessed, for human review.

Do NOT auto-parse into TopPerformer format. Do NOT overwrite src/data/stats.ts.
  </action>
  <verify>
    <automated>cd /Users/mithra_sundaram/Desktop/code/AI/projects/team_site && node -e "const src = require('fs').readFileSync('scripts/scrape-top-performers.js','utf8'); const checks = ['chromium.launch','domcontentloaded','scraped-top-performers-batting.json','scraped-top-performers-bowling.json','writeFileSync','browser.close']; checks.forEach(c => { if(!src.includes(c)) throw new Error('Missing: '+c); }); console.log('script structure ok')"</automated>
  </verify>
  <done>scripts/scrape-top-performers.js exists, targets both batting and bowling ESPNcricinfo KKR stats pages, writes two JSON output files, and handles 403/blocked responses gracefully with a clear warning message.</done>
</task>

</tasks>

<verification>
1. `node -e "const { chromium } = require('playwright')"` exits 0
2. scripts/scrape-season-records.js exists and uses `require('playwright')` (not @playwright/test)
3. scripts/scrape-top-performers.js exists and targets both batting and bowling ESPNcricinfo endpoints
4. Both scripts write to scripts/*.json, never to src/data/
5. `npm run build` still passes (src/data/stats.ts unchanged)
</verification>

<success_criteria>
Playwright installed as devDependency. Both scraping scripts exist and are syntactically valid. Neither script modifies any src/ file. Plan 02 can run both scripts immediately without additional setup.
</success_criteria>

<output>
After completion, create `.planning/phases/05-data-accuracy-and-real-assets/05-01-SUMMARY.md`
</output>
